{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoPathConv (\n",
      "  (upper_layer1): Sequential (\n",
      "    (0): Conv2d(4, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(4, 4), stride=(1, 1), dilation=(1, 1))\n",
      "  )\n",
      "  (upper_layer2): Sequential (\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(2, 2), stride=(1, 1), dilation=(1, 1))\n",
      "  )\n",
      "  (under_layer1): Sequential (\n",
      "    (0): Conv2d(4, 160, kernel_size=(13, 13), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "  )\n",
      "  (final_layer): Conv2d(224, 5, kernel_size=(21, 21), stride=(1, 1))\n",
      ")\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0328\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  0.4779\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      " -0.0311\n",
      "\n",
      "(0 ,3 ,.,.) = \n",
      "  0.3174\n",
      "\n",
      "(0 ,4 ,.,.) = \n",
      "  0.0089\n",
      "[torch.FloatTensor of size 1x5x1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoPathConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoPathConv, self).__init__()\n",
    "        self.upper_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(4,64,7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((4,4),stride = 1)\n",
    "        )\n",
    "        self.upper_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64,64,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2),stride = 1)\n",
    "        )\n",
    "        self.under_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(4,160,13),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final_layer = nn.Conv2d(224,5,21)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        upper_x = self.upper_layer2(self.upper_layer1(x))\n",
    "        under_x = self.under_layer1(x)\n",
    "        #upper_x = F.max_pool2d(F.relu(self.upper_conv1(x)), (4, 4),stride = 1)\n",
    "        #upper_x = F.max_pool2d(F.relu(self.upper_conv2(upper_x)), (2, 2), stride = 1)\n",
    "        #under_x = F.relu(self.under_conv1(x))\n",
    "        final_x = torch.cat((under_x, upper_x), 1)\n",
    "        out = self.final_layer(final_x)\n",
    "        return out\n",
    "        \n",
    "net = TwoPathConv()\n",
    "print(net)\n",
    "\n",
    "x = Variable(torch.randn(1,4,33,33), requires_grad = True)\n",
    "y_pred = net.forward(x)\n",
    "print(y_pred)\n",
    "#y_pred = y_pred.data.resize_(1,5)\n",
    "#y_pred = Variable(y_pred,requires_grad = True)\n",
    "#print(y_pred.size())\n",
    "#print(y_pred.view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase1: 0.011113215213191476%\n",
      "time: 2.6491849999999992\n",
      "averaged time: 0.0026505519999999974\n",
      "phase1: 0.02222643042638295%\n",
      "time: 4.670807\n",
      "averaged time: 0.0023355005\n",
      "phase1: 0.03333964563957443%\n",
      "time: 6.717321999999999\n",
      "averaged time: 0.0022391986666666667\n",
      "phase1: 0.0444528608527659%\n",
      "time: 8.755136\n",
      "averaged time: 0.002188927249999999\n",
      "phase1: 0.05556607606595738%\n",
      "time: 10.813628999999999\n",
      "averaged time: 0.0021628248\n",
      "phase1: 0.06667929127914886%\n",
      "time: 12.846536\n",
      "averaged time: 0.0021411645\n",
      "phase1: 0.07779250649234033%\n",
      "time: 14.750248\n",
      "averaged time: 0.0021072561428571433\n",
      "phase1: 0.0889057217055318%\n",
      "time: 16.643996\n",
      "averaged time: 0.002080573\n",
      "phase1: 0.10001893691872328%\n",
      "time: 18.674301\n",
      "averaged time: 0.0020749549999999994\n",
      "phase1: 0.11113215213191475%\n",
      "time: 20.719751000000002\n",
      "averaged time: 0.0020720135\n",
      "phase 1 data preparation process completed.\n",
      "phase1: 0.012107174158512047%\n",
      "time: 77.414353\n",
      "averaged time: 0.025804980666666665\n",
      "phase1: 0.024214348317024095%\n",
      "time: 83.296902\n",
      "averaged time: 0.013882882333333332\n",
      "phase1: 0.03632152247553614%\n",
      "time: 89.23573\n",
      "averaged time: 0.009915087888888887\n",
      "phase1: 0.04842869663404819%\n",
      "time: 95.33067799999999\n",
      "averaged time: 0.0079442315\n",
      "phase1: 0.06053587079256024%\n",
      "time: 101.433122\n",
      "averaged time: 0.006762233866666667\n",
      "phase1: 0.07264304495107228%\n",
      "time: 107.52623\n",
      "averaged time: 0.005973693666666667\n",
      "phase1: 0.08475021910958433%\n",
      "time: 113.66206600000001\n",
      "averaged time: 0.005412492047619047\n",
      "phase1: 0.09685739326809638%\n",
      "time: 119.748933\n",
      "averaged time: 0.004989550791666667\n",
      "phase1: 0.10896456742660843%\n",
      "time: 126.04853800000001\n",
      "averaged time: 0.004668479148148148\n",
      "phase1: 0.12107174158512048%\n",
      "time: 132.251204\n",
      "averaged time: 0.004408379666666666\n",
      "phase 2 data preparation process completed.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "f5 = h5py.File('training.h5','r')\n",
    "import time\n",
    "#get the training set for phase 1\n",
    "f=open(\"trainval-balanced.txt\", \"r\")\n",
    "content=f.readlines()\n",
    "X_train_phase1=[]\n",
    "y_train_phase1=[]\n",
    "i=0\n",
    "from random import shuffle\n",
    "shuffle(content)\n",
    "prev_time=time.clock()\n",
    "for line in content:\n",
    "    no_n_line=line[0:len(line)-1]\n",
    "    item=no_n_line.split(\" \")\n",
    "    case, x, y, z, l = item[0], int(item[1]), int(item[2]), int(item[3]), int(item[4])\n",
    "    case1 = case[:2]\n",
    "    case2 = case[3:]\n",
    "    X_train_phase1.append(f5[case1][case2][:, x-16:x+17, y-16:y+17, z])\n",
    "    y_train_phase1.append(l)\n",
    "    i+=1\n",
    "    if i%1000==0:\n",
    "        print(\"phase1: \"+str(i*100.0/len(content))+\"%\")\n",
    "        print(\"time: \"+str(time.clock()-prev_time))\n",
    "        print(\"averaged time: \"+str((time.clock()-prev_time)/i))\n",
    "    if i>10000:\n",
    "        break\n",
    "f.close()\n",
    "print (\"phase 1 data preparation process completed.\")\n",
    "\n",
    "#get the training set for phase 2 and the validation set\n",
    "data_val=[]\n",
    "f_in=open(\"trainval.txt\", \"r\")\n",
    "content=f_in.readlines()\n",
    "from random import shuffle\n",
    "shuffle(content)\n",
    "X_train_phase2=[]\n",
    "y_train_phase2=[]\n",
    "X_val=[]\n",
    "y_val=[]\n",
    "i=0\n",
    "for line in content:\n",
    "    no_n_line=line[0:len(line)-1]\n",
    "    item=no_n_line.split(\" \")\n",
    "    case, x, y, z, l = item[0], int(item[1]), int(item[2]), int(item[3]), int(item[4])\n",
    "    case1 = case[:2]\n",
    "    case2 = case[3:]\n",
    "    i += 1\n",
    "    if i%3000==0:\n",
    "        print(\"phase1: \"+str(i*100.0/len(content))+\"%\")\n",
    "        print(\"time: \"+str(time.clock()-prev_time))\n",
    "        print(\"averaged time: \"+str((time.clock()-prev_time)/i))\n",
    "    if i%3000==0:\n",
    "        X_val.append(f5[case1][case2][:, x-16:x+17, y-16:y+17, z])\n",
    "        y_val.append(l)\n",
    "    else:\n",
    "        X_train_phase2.append(f5[case1][case2][:, x-16:x+17, y-16:y+17, z])\n",
    "        y_train_phase2.append(l)\n",
    "    if i>30000:\n",
    "        break\n",
    "    \n",
    "f_in.close()\n",
    "print (\"phase 2 data preparation process completed.\")\n",
    "\n",
    "f5.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n",
      "10001\n",
      "29991\n",
      "29991\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print (len(X_train_phase1))\n",
    "print (len(y_train_phase1))\n",
    "print (len(X_train_phase2))\n",
    "print (len(y_train_phase2))\n",
    "print (len(X_val))\n",
    "print (len(y_val))\n",
    "X_train_phase1 = np.array(X_train_phase1)\n",
    "y_train_phase1 = np.array(y_train_phase1)\n",
    "X_train_phase2 = np.array(X_train_phase2)\n",
    "y_train_phase2 = np.array(y_train_phase2)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print (torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.init as ninit\n",
    "def init_net(net):\n",
    "    for param in net.parameters():\n",
    "        ninit.uniform(param.data, a=-5e-3, b=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batch_phase1(index, batch_size):\n",
    "    start=index*batch_size\n",
    "    end=start+batch_size\n",
    "    X_batch= X_train_phase1[start:end]\n",
    "    y_batch= y_train_phase1[start:end]\n",
    "    X_batch = torch.from_numpy(X_batch)\n",
    "    y_batch = torch.from_numpy(y_batch)\n",
    "    return X_batch, y_batch\n",
    "\n",
    "def create_batch_phase2(index, batch_size):\n",
    "    start=index*batch_size\n",
    "    end=start+batch_size\n",
    "    X_batch= X_train_phase2[start:end]\n",
    "    y_batch= y_train_phase2[start:end]\n",
    "    X_batch = torch.from_numpy(X_batch)\n",
    "    y_batch = torch.from_numpy(y_batch)\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "TwoPathConv (\n",
      "  (upper_layer1): Sequential (\n",
      "    (0): Conv2d(4, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(4, 4), stride=(1, 1), dilation=(1, 1))\n",
      "  )\n",
      "  (upper_layer2): Sequential (\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(2, 2), stride=(1, 1), dilation=(1, 1))\n",
      "  )\n",
      "  (under_layer1): Sequential (\n",
      "    (0): Conv2d(4, 160, kernel_size=(13, 13), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "  )\n",
      "  (final_layer): Conv2d(224, 5, kernel_size=(21, 21), stride=(1, 1))\n",
      ")\n",
      "\n",
      "time used 0.548\n",
      "0.0% completed\n",
      "Variable containing:\n",
      " 1.6142\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Validation accuracy: 0.0\n",
      "\n",
      "time used 1.982\n",
      "22.727272727272727% completed\n",
      "Variable containing:\n",
      " 1.3925\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Validation accuracy: 0.1\n",
      "\n",
      "time used 3.412\n",
      "45.45454545454545% completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "num_train=len(X_train_phase1)\n",
    "num_val=len(X_val)\n",
    "batch_size=900\n",
    "val_size=num_val\n",
    "num_epoch=2.0\n",
    "num_times=int(float(num_train)/batch_size*num_epoch)\n",
    "print (num_times)\n",
    "learning_rate = 5e-3\n",
    "reg=5e-5\n",
    "net = TwoPathConv()\n",
    "print(net)\n",
    "init_net(net)\n",
    "net.cuda(0)\n",
    "\n",
    "#create validation set\n",
    "X_val = torch.from_numpy(np.array(X_val))\n",
    "y_val = torch.from_numpy(np.array(y_val))\n",
    "X_val = Variable(X_val.cuda(0), requires_grad=False)\n",
    "\n",
    "prev_time = time.clock()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=reg)\n",
    "net.zero_grad()\n",
    "for i in range(21):  # loop over the dataset multiple times\n",
    "    X_batch=None\n",
    "    X_batch, y_batch = create_batch_phase1(i, batch_size)\n",
    "    X_batch, y_batch = Variable(X_batch.cuda(0)), Variable(y_batch.cuda(0), requires_grad = False)\n",
    "    y_pred = net.forward(X_batch)\n",
    "    y_pred = y_pred.view(-1,5)\n",
    "    loss = F.cross_entropy(y_pred, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    net.zero_grad()\n",
    "    if i % 5 == 0:\n",
    "        print (\"\")\n",
    "        print ('time used %.3f' % (time.clock()-prev_time))\n",
    "        print (str(float(i)/num_times*100)+\"% completed\")\n",
    "        print (loss)\n",
    "        y_val_pred=net.forward(X_val)\n",
    "        y_val_pred=y_val_pred.view(-1,5)\n",
    "        useless, predicted=torch.max(y_val_pred.data, 1)\n",
    "        correct = (predicted == y_val.cuda(0)).sum()\n",
    "        print('Validation accuracy:', float(correct)/val_size)\n",
    "print (\"phase1 successfully trained!\")\n",
    "#torch.save(net.state_dict(), \"premature_net.txt\")\n",
    "print (\"phase1 successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam(x, dx, config = None):\n",
    "    if config is None: \n",
    "        config = {}\n",
    "        config.setdefault('learning_rate',1e-4)\n",
    "        config.setdefault('beta1',0.9)\n",
    "        config.setdefault('beta2',0.999)\n",
    "        config.setdefault('epsilon',1e-8)\n",
    "        config.setdefault('m',torch.FloatTensor(x.size()).zero_().cuda(0))\n",
    "        config.setdefault('v',torch.FloatTensor(x.size()).zero_().cuda(0))\n",
    "        config.setdefault('t',0)\n",
    "    \n",
    "    next_x = None\n",
    "    config['t'] += 1\n",
    "    x.cuda(0)\n",
    "    dx.cuda(0)\n",
    "    config['m'] = config['beta1'] * config['m'] + (1 - config['beta1']) * dx\n",
    "    config['v'] = config['beta2'] * config['v'] + (1 - config['beta2']) * dx**2\n",
    "    mb = config['m'] / (1 - config['beta1'] ** config['t'])\n",
    "    vb = config['v'] / (1 - config['beta2'] ** config['t'])\n",
    "    next_x = x - config['learning_rate'] * mb / (np.sqrt(vb) + config['epsilon'])\n",
    "    return next_x, config\n",
    "\n",
    "def sgd_momentum(w, dw, config = None):\n",
    "\n",
    "    if config is None: \n",
    "        config = {}\n",
    "        config.setdefault('learning_rate', 5e-4)\n",
    "        config.setdefault('momentum', 0.9)\n",
    "\n",
    "    v = config.get('velocity', torch.FloatTensor(w.size()).zero_().cuda(0))\n",
    "    next_w = None\n",
    "\n",
    "    v = config['momentum'] * v - config['learning_rate'] * dw\n",
    "    next_w = w + v\n",
    "    config['velocity'] = v\n",
    "\n",
    "    return next_w, config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "num_train=len(data_train_phase2)\n",
    "num_val=len(data_val)\n",
    "batch_size=900\n",
    "val_size=num_val\n",
    "num_epoch=2.0\n",
    "num_times=int(float(num_train)/batch_size*num_epoch)\n",
    "print (num_times)\n",
    "learning_rate = 5e-4\n",
    "reg=5e-5\n",
    "net.load_state_dict(torch.load(\"premature_net.txt\"))\n",
    "net.cuda(0)\n",
    "\n",
    "prev_time = time.clock()\n",
    "config_weight = None\n",
    "config_bias = None\n",
    "net.zero_grad()\n",
    "for i in xrange(21):  # loop over the dataset multiple times\n",
    "    X_batch = None\n",
    "    batch_time=time.clock()\n",
    "    X_batch, y_batch = create_batch_phase2(i, batch_size)\n",
    "    print(\"batch_time: \"+ str(time.clock()-batch_time))\n",
    "    X_batch, y_batch = Variable(X_batch.cuda(0)), Variable(y_batch.cuda(0), requires_grad = False)\n",
    "    y_pred = net.forward(X_batch)\n",
    "    y_pred = y_pred.view(-1,5)\n",
    "    loss = F.cross_entropy(y_pred, y_batch)\n",
    "    loss.backward()\n",
    "    dweight = net.final_layer.weight.grad.data+reg*net.final_layer.weight.data\n",
    "    dbias = net.final_layer.bias.grad.data\n",
    "    net.final_layer.weight.data, config_weight = sgd_momentum(net.final_layer.weight.data, dweight, config_weight)\n",
    "    net.final_layer.bias.data, config_bias = sgd_momentum(net.final_layer.bias.data, dbias, config_bias)\n",
    "    net.zero_grad()\n",
    "    if i % 5 == 0:\n",
    "        print (\"\")\n",
    "        print ('time used %.3f' % (time.clock()-prev_time))\n",
    "        print (str(float(i)/num_times*100)+\"% completed\")\n",
    "        print (loss)\n",
    "        y_val_pred=net.forward(X_val)\n",
    "        y_val_pred=y_val_pred.view(-1,5)\n",
    "        useless, predicted=torch.max(y_val_pred.data, 1)\n",
    "        correct = (predicted == y_val.cuda(0)).sum()\n",
    "        print('Validation accuracy:', float(correct)/val_size)\n",
    "print (\"phase2 successfully trained!\")\n",
    "#torch.save(net.state_dict(), \"premature_net_phase2_without_biasreg.txt\")\n",
    "print (\"phase2 successfully saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
