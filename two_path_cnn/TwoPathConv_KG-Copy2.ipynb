{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoPathConv (\n",
      "  (upper_layer1): Sequential (\n",
      "    (0): Conv2d(4, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(4, 4), stride=(1, 1), dilation=(1, 1))\n",
      "  )\n",
      "  (upper_layer2): Sequential (\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(2, 2), stride=(1, 1), dilation=(1, 1))\n",
      "  )\n",
      "  (under_layer1): Sequential (\n",
      "    (0): Conv2d(4, 160, kernel_size=(13, 13), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "  )\n",
      "  (final_layer): Conv2d(224, 5, kernel_size=(21, 21), stride=(1, 1))\n",
      ")\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      " -2.0225e-03  2.0103e-03 -8.7108e-04  ...   2.5124e-03  1.0436e-03  2.7271e-03\n",
      " -3.2380e-04 -2.3371e-03 -2.4914e-03  ...  -9.3090e-04  2.3428e-04  2.9733e-04\n",
      " -1.1450e-03 -2.5839e-03  5.4721e-05  ...   1.8655e-03 -2.5009e-03 -3.0827e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  8.5974e-04 -2.9496e-04 -3.7688e-04  ...   1.9733e-03 -2.1250e-03  2.5020e-03\n",
      "  1.0631e-03  2.7537e-03 -7.1914e-04  ...   2.0615e-03 -3.1140e-03 -1.5810e-03\n",
      "  2.3190e-03 -2.3291e-03  8.6752e-05  ...  -9.9066e-04 -8.5231e-04  1.0385e-04\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      " -6.6543e-04 -2.7918e-03  1.9029e-03  ...  -1.9390e-03  1.4895e-03  1.2964e-03\n",
      "  6.1397e-04 -8.3859e-04  1.3868e-03  ...  -1.7944e-03  7.0295e-04  2.0263e-03\n",
      "  2.6375e-03 -3.0457e-03  1.0223e-03  ...  -2.8936e-03 -1.3998e-04  8.1311e-04\n",
      "                 ...                   ⋱                   ...                \n",
      " -6.5141e-04  3.6542e-04 -9.3974e-04  ...   6.4709e-04 -4.9420e-04 -6.8198e-04\n",
      "  2.7612e-03  2.5478e-03  2.8959e-03  ...   2.6935e-03 -8.9978e-04  1.0991e-03\n",
      "  1.0541e-03  2.4402e-03 -2.9394e-04  ...  -1.7294e-03 -3.6377e-05 -9.7374e-04\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      "  1.7039e-03 -5.1669e-04  1.4232e-03  ...   2.9468e-05  2.3729e-03  6.8915e-04\n",
      "  2.7114e-03  6.8873e-04 -1.7659e-03  ...   1.4492e-03 -1.6269e-03  4.9037e-04\n",
      " -2.8660e-03 -1.4558e-03 -2.6802e-03  ...  -1.1118e-03  4.4312e-04 -2.3828e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  2.0974e-03  2.0045e-03  1.1976e-03  ...   1.2876e-03 -8.1761e-05 -3.0972e-03\n",
      " -2.3798e-03  1.6983e-04  5.8239e-04  ...  -2.7238e-03  2.5973e-03  1.9742e-04\n",
      " -2.6135e-03 -3.1107e-03  1.2467e-03  ...   2.5794e-03  3.0796e-03 -2.1519e-03\n",
      "    ... \n",
      "\n",
      "( 0 ,221,.,.) = \n",
      "  1.3463e-03  2.4214e-03  2.5458e-03  ...  -2.9347e-03  3.0378e-03  4.1524e-04\n",
      " -1.3135e-03  1.3835e-03 -2.7140e-05  ...  -1.7834e-03  1.1689e-03 -1.4293e-03\n",
      "  2.7418e-03 -2.4437e-03 -3.6445e-04  ...   1.5781e-03  2.1127e-03 -2.7195e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  2.4963e-03 -2.7449e-03  1.1803e-03  ...   5.4427e-04 -1.0726e-03 -2.6595e-03\n",
      " -1.8846e-03 -1.3814e-03  4.3005e-05  ...   3.5676e-04  1.1654e-03 -2.7866e-03\n",
      "  1.0922e-03 -1.6526e-03 -1.6637e-04  ...  -1.3039e-03 -3.2306e-04  2.7448e-03\n",
      "\n",
      "( 0 ,222,.,.) = \n",
      " -9.9979e-04  2.2822e-03 -2.7911e-03  ...  -1.6648e-03  1.3568e-03 -2.8683e-03\n",
      " -9.0394e-04 -7.3869e-04  6.1791e-04  ...   3.1004e-03 -8.8586e-04  1.1687e-03\n",
      " -1.2115e-04 -2.6666e-03  2.8052e-03  ...  -9.5279e-04  1.7140e-03  8.7054e-05\n",
      "                 ...                   ⋱                   ...                \n",
      " -2.9405e-03 -1.0093e-03  1.1529e-03  ...   8.2610e-04  1.5561e-03 -2.0880e-03\n",
      "  2.6052e-04 -2.1247e-03  3.1349e-03  ...   1.4464e-04 -1.6858e-03 -1.1497e-03\n",
      " -2.5037e-03  2.5344e-03 -2.2220e-03  ...   2.1100e-03 -2.4494e-03  1.6721e-03\n",
      "\n",
      "( 0 ,223,.,.) = \n",
      " -2.2631e-04  1.9306e-03  1.6999e-03  ...  -1.7858e-03 -1.0932e-04  3.7640e-04\n",
      "  3.0046e-03  1.5956e-03 -4.0523e-04  ...   2.9001e-04 -3.1429e-03  1.9176e-03\n",
      "  3.9611e-04 -2.4794e-03 -2.5113e-03  ...   7.8803e-04 -7.1457e-04  2.0100e-03\n",
      "                 ...                   ⋱                   ...                \n",
      " -3.2808e-06  6.7397e-04 -1.0775e-03  ...   2.7783e-03  2.5309e-03 -1.7621e-03\n",
      " -2.7457e-03  2.4181e-03  1.3336e-03  ...  -2.7296e-03  9.5588e-04 -2.7292e-03\n",
      " -2.6038e-03 -8.1378e-05  5.6449e-04  ...  -1.0080e-03 -5.8878e-04 -7.9286e-04\n",
      "      ⋮  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      " -2.9270e-03 -4.8631e-05 -1.2599e-03  ...   7.5368e-04  2.3871e-03  6.6446e-04\n",
      " -2.9966e-03 -4.5155e-04 -1.7434e-03  ...   6.2077e-04  2.0762e-03 -1.8596e-04\n",
      "  2.1274e-04  6.7679e-04 -2.1536e-03  ...   3.0372e-03  2.8555e-03 -1.5474e-03\n",
      "                 ...                   ⋱                   ...                \n",
      " -2.8020e-03  1.1344e-03 -2.2574e-03  ...  -1.1679e-03 -2.4063e-03 -3.1512e-04\n",
      "  1.1715e-03 -6.9172e-04  2.7441e-03  ...   8.1529e-04 -4.4938e-04 -3.0601e-03\n",
      " -1.0777e-03 -1.0183e-03  7.2648e-04  ...  -4.9064e-04  8.6725e-04  1.2862e-04\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      " -1.7314e-03 -2.6866e-03  4.3386e-04  ...   1.8639e-04 -8.3816e-04 -6.9145e-04\n",
      " -7.1208e-04  2.2297e-03  2.4331e-03  ...   6.2881e-05 -1.7481e-04 -2.1016e-05\n",
      " -1.1448e-03  1.3010e-03  2.7082e-03  ...  -8.9242e-04  1.2010e-03  1.3374e-03\n",
      "                 ...                   ⋱                   ...                \n",
      " -2.5747e-03  1.5665e-03  1.0715e-03  ...   6.4847e-04 -1.9063e-03  2.6942e-03\n",
      " -2.7070e-03  1.7164e-03  1.0258e-03  ...  -3.0770e-03 -1.8280e-03 -2.4670e-03\n",
      " -2.0333e-03 -7.5241e-04  1.8942e-03  ...  -2.9100e-05 -3.1620e-03 -1.2641e-03\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      "  2.0975e-03  2.3415e-03  3.0813e-03  ...   2.3639e-03 -3.1235e-03  1.3971e-03\n",
      " -3.4068e-04  1.9807e-04 -2.1940e-03  ...   1.5832e-03  2.6860e-03 -2.5078e-03\n",
      " -6.0881e-04  2.0819e-03  2.8466e-03  ...   2.8161e-03 -2.1106e-03  1.4449e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  3.3239e-04 -2.6954e-03  3.0900e-05  ...  -2.7283e-03 -1.5849e-04 -1.6160e-03\n",
      " -2.6796e-03  1.7335e-03  2.0844e-03  ...   8.1702e-04  2.1490e-03 -1.1767e-03\n",
      "  3.1236e-03 -1.7289e-03  1.3561e-03  ...   1.8493e-03  1.1774e-03 -2.4924e-03\n",
      "    ... \n",
      "\n",
      "( 1 ,221,.,.) = \n",
      " -6.7391e-04  4.3289e-04 -2.4902e-03  ...   1.0036e-03  1.9597e-03 -2.5099e-03\n",
      "  1.2859e-03 -6.2271e-04  2.0275e-03  ...  -4.1928e-04 -1.9730e-03 -1.1642e-03\n",
      "  9.7770e-04  2.4552e-03 -1.5570e-03  ...  -1.6627e-03  2.6593e-03  9.2780e-04\n",
      "                 ...                   ⋱                   ...                \n",
      "  2.4797e-03 -1.3961e-03  1.7003e-03  ...   5.2810e-04  1.1035e-03  2.5534e-03\n",
      "  3.9078e-04  7.7585e-04 -3.1231e-03  ...  -2.7288e-03 -3.1536e-03 -1.1127e-03\n",
      "  5.4066e-04  2.0243e-03 -1.8087e-03  ...   1.1925e-03  2.5488e-03  8.4571e-04\n",
      "\n",
      "( 1 ,222,.,.) = \n",
      " -1.4612e-03 -1.2148e-03 -1.3374e-03  ...   1.5995e-03 -1.9608e-03 -2.3453e-03\n",
      "  1.7262e-03 -1.3363e-03 -1.0285e-03  ...   2.9763e-03  2.5773e-03  1.3169e-03\n",
      " -2.8581e-03  1.8741e-03 -5.9641e-04  ...  -2.3370e-03  1.0690e-03  7.5977e-04\n",
      "                 ...                   ⋱                   ...                \n",
      " -4.5452e-04 -1.3804e-03 -3.0761e-03  ...  -1.0240e-03  2.1450e-03  1.1139e-03\n",
      " -3.1326e-04  2.3471e-03 -2.6207e-03  ...   5.6686e-04  3.7291e-04 -3.9227e-06\n",
      " -1.9117e-03 -2.5538e-03 -6.8804e-04  ...   1.5552e-03  8.8172e-04 -1.6807e-03\n",
      "\n",
      "( 1 ,223,.,.) = \n",
      " -1.5420e-03  1.0019e-03 -7.0160e-04  ...  -1.6365e-03 -2.0950e-03 -6.8546e-04\n",
      "  2.6738e-03  3.0129e-03  2.5033e-03  ...   2.2145e-03 -2.1347e-03 -1.7647e-03\n",
      "  1.7666e-03  1.0057e-03  1.1130e-03  ...  -2.7124e-03  1.9001e-03 -3.1105e-03\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.4443e-03  3.0658e-03  8.5889e-04  ...   2.4930e-03 -1.6392e-03 -7.2539e-04\n",
      " -3.0156e-03 -2.7933e-04  1.1558e-03  ...  -1.4362e-03  2.5310e-03  1.1280e-03\n",
      "  3.3609e-04 -6.2616e-04 -2.7961e-04  ...   1.2018e-03  1.3976e-03 -3.2981e-04\n",
      "      ⋮  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      " -1.3126e-03 -2.9972e-03  1.9156e-03  ...   5.6758e-04 -2.4168e-03 -2.0524e-03\n",
      " -9.5520e-04 -5.3622e-04 -6.6583e-04  ...  -2.4050e-03 -9.4190e-04 -9.7448e-04\n",
      "  2.7266e-03 -1.8017e-03  1.6254e-03  ...   1.5299e-03  1.1406e-03  2.8937e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  2.6640e-03 -2.2502e-03  2.6803e-03  ...   2.6536e-04  1.8521e-03 -1.6889e-03\n",
      "  6.6121e-04  1.7150e-03  2.2626e-03  ...   2.9617e-03 -1.0347e-03 -4.9209e-04\n",
      " -1.6515e-03  1.9828e-03 -1.0089e-03  ...   1.3703e-04  4.7733e-04 -1.7933e-03\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      "  4.3090e-04  1.7875e-03 -2.8024e-03  ...   1.7363e-03  1.5922e-03  2.5047e-03\n",
      " -4.6982e-04  1.9407e-03 -2.9604e-04  ...   4.8417e-04  1.9971e-03 -1.0696e-03\n",
      "  1.2518e-03  4.9404e-04 -3.1456e-03  ...   3.9458e-04  2.0437e-03 -1.2880e-03\n",
      "                 ...                   ⋱                   ...                \n",
      " -2.7414e-03  1.9506e-03 -2.5861e-03  ...  -1.4326e-03  1.5578e-03 -1.5812e-03\n",
      "  2.1804e-04  1.0367e-03 -1.0642e-03  ...  -1.1922e-03 -1.4283e-03  1.5948e-03\n",
      " -2.3652e-03  1.1983e-03  1.6897e-03  ...  -2.2803e-03  1.6287e-03 -2.3126e-03\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      "  1.0905e-03 -1.6557e-03 -1.3061e-03  ...   2.4549e-03  3.1467e-03  2.6707e-03\n",
      " -1.2576e-03 -1.0471e-03  1.4659e-03  ...   2.8518e-03  8.5490e-04  5.9772e-04\n",
      "  2.2417e-03 -1.0536e-03  1.2671e-03  ...   2.1810e-04  1.5676e-03 -2.1724e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  1.8000e-03  2.3159e-03  3.4331e-04  ...  -2.2147e-03  1.4777e-03 -1.2595e-03\n",
      "  3.2586e-04  1.9006e-03 -1.7105e-03  ...   1.5325e-03  2.8315e-03 -2.4109e-03\n",
      " -2.7947e-03 -1.5690e-03 -3.1585e-04  ...   1.3934e-04  2.2021e-03 -1.1690e-03\n",
      "    ... \n",
      "\n",
      "( 2 ,221,.,.) = \n",
      "  5.5839e-04  1.7646e-03  2.6614e-03  ...  -7.8880e-04  1.3956e-03 -2.2515e-03\n",
      " -2.5500e-03  2.2356e-03 -1.1774e-03  ...   1.9805e-03  2.3885e-05 -3.1183e-03\n",
      "  2.6472e-03  1.9906e-03  2.4759e-03  ...  -2.0892e-03 -2.8706e-04 -2.3501e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  2.5853e-04  5.9360e-04  5.9544e-04  ...   7.2768e-04  1.4851e-03  2.8604e-03\n",
      " -3.0825e-03 -2.3799e-03  1.5652e-03  ...  -1.0303e-03 -2.2021e-03  6.1101e-04\n",
      "  2.3932e-03 -1.6816e-03 -3.1227e-03  ...  -1.1821e-03 -1.8379e-03  2.6746e-03\n",
      "\n",
      "( 2 ,222,.,.) = \n",
      "  5.2448e-04 -6.5725e-04 -5.5673e-04  ...  -2.4789e-03 -1.4943e-03 -1.4933e-03\n",
      " -5.5987e-04  4.2274e-04  2.0942e-03  ...   9.8483e-04 -1.9725e-03  8.8188e-04\n",
      " -2.5959e-04  6.6467e-04  1.4675e-03  ...  -7.3471e-04 -1.4168e-03  2.0080e-03\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.4504e-04  2.0584e-03  2.1310e-03  ...  -9.9621e-04 -6.6479e-04 -8.2481e-04\n",
      " -1.3306e-04  2.6871e-03  9.6502e-04  ...  -5.8307e-04  4.3169e-04  1.6837e-03\n",
      "  1.9616e-03  4.0134e-05  1.6649e-04  ...  -2.3347e-03  3.1675e-03 -9.7608e-04\n",
      "\n",
      "( 2 ,223,.,.) = \n",
      " -2.2222e-03  2.3560e-04 -1.7092e-04  ...   1.9014e-03 -1.4987e-03 -2.8695e-03\n",
      " -4.0763e-04 -3.1417e-03  1.9082e-03  ...  -9.1918e-04  1.2634e-03 -9.5771e-04\n",
      "  2.4379e-03  9.1475e-04  2.8442e-03  ...  -2.3107e-03 -2.0143e-03 -1.1870e-03\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.8658e-03 -2.7961e-03 -2.5940e-03  ...   1.3003e-03 -2.2043e-03 -2.5077e-03\n",
      " -5.4169e-04  3.0032e-03 -3.1482e-03  ...  -2.2279e-03  2.3047e-03  1.3967e-03\n",
      " -1.7342e-04  3.1182e-03 -2.1641e-03  ...  -4.6563e-04  1.7835e-03  1.3621e-03\n",
      "      ⋮  \n",
      "\n",
      "( 3 , 0 ,.,.) = \n",
      "  8.7968e-04 -7.1918e-04  3.9747e-04  ...   1.6349e-03  1.0075e-03  2.4341e-03\n",
      " -4.5115e-04 -1.4149e-03 -1.4101e-03  ...   1.2333e-03 -1.3377e-03 -2.7833e-03\n",
      " -1.8718e-03  2.2615e-05  1.8601e-03  ...   7.8113e-04 -1.4533e-03  3.1156e-03\n",
      "                 ...                   ⋱                   ...                \n",
      " -2.5647e-03  3.8902e-04  3.0806e-03  ...   1.0694e-03 -2.2108e-03 -3.8291e-04\n",
      " -2.4812e-03 -2.2034e-03  2.5055e-03  ...   2.1702e-03 -8.0941e-04 -2.7770e-04\n",
      "  2.3814e-03  1.3888e-03 -3.1327e-03  ...  -2.5524e-04  2.2827e-03 -2.8569e-03\n",
      "\n",
      "( 3 , 1 ,.,.) = \n",
      "  1.9264e-03  1.3792e-03 -4.4312e-04  ...   2.5036e-03 -1.7189e-03 -3.1577e-03\n",
      "  6.6699e-04 -1.4684e-03 -4.8586e-04  ...   2.5940e-03  2.6694e-03  2.6948e-03\n",
      "  3.2104e-04  1.2087e-03  1.1294e-03  ...  -2.6069e-03 -1.8238e-03  2.3540e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  2.7981e-03 -2.5044e-03 -2.6244e-03  ...  -2.1060e-03  2.3087e-03  3.6926e-04\n",
      " -3.0268e-03  4.9801e-04  9.2111e-04  ...   4.6283e-04 -8.3400e-04  1.7113e-03\n",
      "  3.1224e-03 -2.4906e-03 -2.5414e-04  ...   2.0428e-03 -7.2230e-04  1.9278e-03\n",
      "\n",
      "( 3 , 2 ,.,.) = \n",
      " -1.8952e-04  6.0184e-04 -1.7890e-04  ...   5.4133e-04  2.4927e-03 -9.5219e-04\n",
      "  2.4962e-03 -2.5068e-03  5.5164e-04  ...   3.0464e-03 -1.5205e-03 -2.9954e-03\n",
      "  1.7734e-03  1.3843e-04  8.3286e-04  ...   1.4282e-03  1.8472e-03 -1.3783e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  2.3648e-03  4.7011e-04 -5.2672e-04  ...  -9.0599e-04 -4.2393e-04  7.7989e-04\n",
      "  1.9135e-03 -1.6862e-03  2.0447e-03  ...   1.1333e-03  1.4707e-03  5.7793e-04\n",
      " -2.7471e-03  2.0168e-03 -1.5841e-03  ...  -3.0246e-04 -2.5678e-03 -2.8237e-03\n",
      "    ... \n",
      "\n",
      "( 3 ,221,.,.) = \n",
      " -2.2743e-03  1.9516e-03 -2.0501e-03  ...   1.4621e-03 -1.6839e-03  1.3860e-03\n",
      " -1.6158e-03 -2.6229e-04  3.1309e-03  ...   2.1474e-03 -2.3494e-04  1.1618e-04\n",
      " -5.9863e-04  1.8075e-03 -3.1204e-03  ...  -2.2000e-03 -2.6814e-03  1.5864e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  1.9248e-03  1.8040e-03  2.3887e-03  ...   8.1573e-04  2.3601e-03  2.1870e-03\n",
      "  3.1283e-03 -9.4454e-04 -1.1985e-03  ...   6.8311e-04 -1.2424e-03  1.0150e-03\n",
      " -8.9677e-05  2.2587e-03 -1.7601e-03  ...  -2.2502e-03 -1.2781e-03 -6.5288e-04\n",
      "\n",
      "( 3 ,222,.,.) = \n",
      "  2.1747e-03  8.7450e-04  2.4056e-03  ...   2.5930e-03  1.2214e-03 -1.7587e-03\n",
      "  3.5475e-04 -1.0086e-03 -2.2766e-03  ...   4.1963e-04  1.1883e-03  2.9367e-03\n",
      " -2.9833e-03  3.0729e-03  2.6379e-03  ...  -1.2452e-04  5.6892e-04  1.1891e-03\n",
      "                 ...                   ⋱                   ...                \n",
      " -3.1678e-03 -2.9306e-03  2.2238e-03  ...   2.7014e-03 -1.9661e-03 -1.1052e-03\n",
      "  2.6943e-03 -1.7616e-03 -1.3774e-03  ...  -3.0511e-03 -3.5391e-04 -1.9706e-03\n",
      " -2.5223e-03  8.9882e-05  9.9830e-04  ...   7.6644e-04 -1.8657e-03  2.8935e-03\n",
      "\n",
      "( 3 ,223,.,.) = \n",
      " -3.0588e-03  2.5521e-03  4.3090e-04  ...   3.0080e-03  1.2427e-03  1.2007e-03\n",
      "  1.5241e-03 -1.6206e-03  5.1532e-04  ...   1.9588e-03  1.6799e-04 -8.2394e-05\n",
      "  2.4464e-03 -1.6958e-03  1.8366e-03  ...  -3.3453e-04 -2.9185e-03  7.1276e-04\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.7056e-03 -3.1388e-03  3.9966e-04  ...   1.2674e-03  1.7551e-03  1.3794e-03\n",
      " -1.2225e-03  4.6762e-05  1.2924e-03  ...   3.6360e-04 -7.6869e-04  9.7162e-04\n",
      "  1.4135e-03 -2.2782e-03  2.9990e-03  ...   2.7796e-05  3.2663e-04 -3.1216e-03\n",
      "      ⋮  \n",
      "\n",
      "( 4 , 0 ,.,.) = \n",
      " -2.0413e-03 -2.0319e-03 -2.1849e-03  ...   1.9339e-03  1.4418e-03 -1.7446e-03\n",
      " -1.5696e-03  6.8534e-04 -1.5390e-03  ...   5.2906e-04  1.3442e-03 -1.0580e-03\n",
      " -2.9316e-03  1.7113e-04  4.8336e-04  ...  -1.2680e-03  7.4108e-04  3.1370e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  2.1034e-03  2.5786e-03  1.5361e-03  ...   2.9864e-03 -8.0565e-04 -2.3414e-03\n",
      "  1.6424e-03  1.8179e-03 -2.0863e-03  ...   1.3769e-03  2.2292e-03  5.9771e-04\n",
      "  5.2934e-04 -1.7725e-03 -4.4126e-04  ...  -1.7723e-03 -1.1327e-03 -1.1286e-03\n",
      "\n",
      "( 4 , 1 ,.,.) = \n",
      "  3.0005e-03 -2.5380e-03 -2.2581e-03  ...  -2.5998e-03 -2.6760e-03 -2.5045e-03\n",
      " -2.2088e-04 -4.5332e-04 -2.1344e-03  ...   2.5147e-03 -1.9215e-03 -3.9718e-04\n",
      " -2.4043e-03  6.5223e-04 -4.6355e-04  ...   2.1951e-03  7.6700e-04  2.7169e-03\n",
      "                 ...                   ⋱                   ...                \n",
      " -7.5259e-05 -8.6004e-04 -1.9676e-03  ...  -1.1168e-03  2.9748e-03 -2.9585e-03\n",
      "  2.1203e-03 -4.3075e-04  1.3954e-03  ...  -3.1443e-03  1.8164e-03 -3.5145e-04\n",
      "  2.5987e-03 -2.9413e-03  1.6461e-03  ...  -2.4065e-03  3.0179e-03  2.9046e-03\n",
      "\n",
      "( 4 , 2 ,.,.) = \n",
      "  9.1955e-05 -2.7201e-03  3.2874e-04  ...  -3.0948e-04 -2.3464e-03  2.3088e-04\n",
      " -1.6374e-03  1.0190e-03 -1.0820e-03  ...   1.7804e-03  1.1333e-03  1.3844e-03\n",
      "  1.1105e-03  1.2820e-03  1.4458e-03  ...   4.5151e-04 -2.6833e-03 -2.4992e-03\n",
      "                 ...                   ⋱                   ...                \n",
      " -5.2173e-04  1.4187e-03  7.0645e-04  ...  -1.3502e-03  2.9605e-03  1.2463e-03\n",
      " -2.4325e-03 -1.5431e-03  1.5809e-03  ...  -2.9134e-03  1.5740e-03 -1.9625e-04\n",
      "  9.5656e-04  8.2751e-04  1.8487e-03  ...   2.8302e-03 -2.3969e-03  1.7240e-03\n",
      "    ... \n",
      "\n",
      "( 4 ,221,.,.) = \n",
      "  1.0020e-03 -1.0299e-03 -2.0652e-04  ...   1.2241e-05 -2.3898e-03  5.5238e-04\n",
      " -1.3973e-03 -2.2581e-04 -2.8586e-03  ...   1.0032e-03  2.6498e-03 -2.8246e-03\n",
      " -4.8631e-04  8.0591e-04 -1.9698e-03  ...   1.4267e-03  2.9219e-03 -2.0240e-03\n",
      "                 ...                   ⋱                   ...                \n",
      " -3.4513e-04  1.2111e-03 -4.3002e-04  ...  -9.5952e-04 -1.7228e-03 -1.3567e-04\n",
      " -1.0157e-03  8.0084e-04 -2.6702e-04  ...   1.4373e-03  1.2152e-03  2.0336e-03\n",
      " -2.8508e-03 -1.4762e-03 -1.1015e-03  ...  -3.1638e-03  3.0070e-03  1.6838e-03\n",
      "\n",
      "( 4 ,222,.,.) = \n",
      " -6.5953e-04 -2.8417e-03  2.6126e-04  ...   1.1875e-03  4.0721e-04 -2.8137e-03\n",
      "  2.3960e-03 -1.0101e-03  1.5379e-03  ...   4.0135e-04  6.1517e-05 -1.6231e-03\n",
      " -1.2037e-03  1.0536e-03 -1.9496e-04  ...  -4.5924e-04  1.6546e-03  3.1478e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  1.2568e-03  1.9142e-03 -1.3069e-03  ...  -2.7852e-04  1.6248e-03 -1.9635e-03\n",
      "  2.4146e-03 -1.7526e-03 -2.5890e-03  ...   4.0974e-04  6.0696e-04 -2.5176e-03\n",
      " -1.8952e-03 -1.9324e-03 -1.2084e-03  ...   1.7015e-03  1.1887e-03  2.1021e-03\n",
      "\n",
      "( 4 ,223,.,.) = \n",
      " -2.5227e-03  2.8626e-04  2.9152e-04  ...   9.8391e-04 -2.9935e-03 -2.7772e-03\n",
      "  7.9877e-04 -1.2612e-03 -8.9271e-04  ...  -3.5230e-04  1.7140e-03  1.9306e-03\n",
      "  4.8299e-04 -8.9246e-04 -2.2696e-04  ...  -1.6305e-03  2.5118e-03 -2.9162e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  1.7613e-03  8.4253e-04 -2.7219e-03  ...  -5.6169e-04  2.8355e-03 -5.8010e-04\n",
      "  3.1325e-03  1.5092e-04 -3.3632e-04  ...   2.9336e-03 -1.4187e-03 -1.6960e-03\n",
      " -2.7604e-03 -1.4506e-03 -1.5298e-03  ...   2.6486e-03 -1.1723e-03  1.3635e-03\n",
      "[torch.FloatTensor of size 5x224x21x21]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-03 *\n",
      " -3.1414\n",
      "  0.3816\n",
      " -1.5978\n",
      "  1.9430\n",
      " -2.2497\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoPathConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoPathConv, self).__init__()\n",
    "        self.upper_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(4,64,7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((4,4),stride = 1)\n",
    "        )\n",
    "        self.upper_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64,64,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2),stride = 1)\n",
    "        )\n",
    "        self.under_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(4,160,13),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final_layer = nn.Conv2d(224,5,21)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        upper_x = self.upper_layer2(self.upper_layer1(x))\n",
    "        under_x = self.under_layer1(x)\n",
    "        #upper_x = F.max_pool2d(F.relu(self.upper_conv1(x)), (4, 4),stride = 1)\n",
    "        #upper_x = F.max_pool2d(F.relu(self.upper_conv2(upper_x)), (2, 2), stride = 1)\n",
    "        #under_x = F.relu(self.under_conv1(x))\n",
    "        final_x = torch.cat((under_x, upper_x), 1)\n",
    "        out = self.final_layer(final_x)\n",
    "        return out\n",
    "        \n",
    "net = TwoPathConv()\n",
    "print(net)\n",
    "\n",
    "x = Variable(torch.randn(1,4,33,33), requires_grad = True)\n",
    "y_pred = net.forward(x)\n",
    "for item in list(net.final_layer.parameters()):\n",
    "    print(item)\n",
    "#y_pred = y_pred.data.resize_(1,5)\n",
    "#y_pred = Variable(y_pred,requires_grad = True)\n",
    "#print(y_pred.size())\n",
    "#print(y_pred.view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase 1 data preparation process completed.\n",
      "phase 2 data preparation process completed.\n"
     ]
    }
   ],
   "source": [
    "#get the training set for phase 1\n",
    "f=open(\"trainval-balanced.txt\", \"r\")\n",
    "content=f.readlines()\n",
    "data=[]\n",
    "data_train_phase1=[]\n",
    "i=0\n",
    "for line in content:\n",
    "    no_n_line=line[0:len(line)-1]\n",
    "    item=no_n_line.split(\" \")\n",
    "    if item[0]!=\"HG/0001\":\n",
    "        data.append([])\n",
    "        data[i].append(item[0])\n",
    "        data[i].append(int(item[1]))\n",
    "        data[i].append(int(item[2]))\n",
    "        data[i].append(int(item[3]))\n",
    "        data[i].append(int(item[4]))\n",
    "        data_train_phase1.append(data[i])\n",
    "        i += 1\n",
    "f.close()\n",
    "from random import shuffle\n",
    "\n",
    "print (\"phase 1 data preparation process completed.\")\n",
    "\n",
    "#get the training set for phase 2 and the validation set\n",
    "data_val=[]\n",
    "f_in=open(\"trainval.txt\", \"r\")\n",
    "content=f_in.readlines()\n",
    "data=[]\n",
    "data_val=[]\n",
    "data_train_phase2=[]\n",
    "i=0\n",
    "for line in content:\n",
    "    no_n_line=line[0:len(line)-1]\n",
    "    item=no_n_line.split(\" \")\n",
    "    if item[0]!=\"HG/0001\":\n",
    "        data.append([])\n",
    "        data[i].append(item[0])\n",
    "        data[i].append(int(item[1]))\n",
    "        data[i].append(int(item[2]))\n",
    "        data[i].append(int(item[3]))\n",
    "        data[i].append(int(item[4]))\n",
    "        if i%30000==0:\n",
    "            data_val.append(data[i])\n",
    "        else:\n",
    "            data_train_phase2.append(data[i])\n",
    "        i += 1\n",
    "f_in.close()\n",
    "print (\"phase 2 data preparation process completed.\")\n",
    "\n",
    "\n",
    "import h5py\n",
    "f = h5py.File('training.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23709634\n",
      "791\n",
      "8629298\n",
      "23708843\n"
     ]
    }
   ],
   "source": [
    "print (len(data))\n",
    "print (len(data_val))\n",
    "print (len(data_train_phase1))\n",
    "print (len(data_train_phase2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print (torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.init as ninit\n",
    "def init_net(net):\n",
    "    for param in net.parameters():\n",
    "        ninit.uniform(param.data, a=-5e-3, b=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batch_phase1(index, batch_size):\n",
    "    step=len(data_train_phase1)//batch_size\n",
    "    starting_index=index%step\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    while starting_index < len(data_train_phase1):\n",
    "        case, x, y, z, l = data_train_phase1[starting_index]\n",
    "        case1 = case[:2]\n",
    "        case2 = case[3:]\n",
    "        X_batch.append(f[case1][case2][:, x-16:x+17, y-16:y+17, z])\n",
    "        y_batch.append(l)\n",
    "        starting_index+=step\n",
    "    X_batch = torch.from_numpy(np.array(X_batch))\n",
    "    y_batch = torch.from_numpy(np.array(y_batch))\n",
    "    return X_batch, y_batch\n",
    "\n",
    "def create_batch_phase2(index, batch_size):\n",
    "    step=len(data_train_phase2)//batch_size\n",
    "    starting_index=index%step\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    while starting_index < len(data_train_phase2):\n",
    "        case, x, y, z, l = data_train_phase2[starting_index]\n",
    "        case1 = case[:2]\n",
    "        case2 = case[3:]\n",
    "        X_batch.append(f[case1][case2][:, x-16:x+17, y-16:y+17, z])\n",
    "        y_batch.append(l)\n",
    "        starting_index+=step\n",
    "    X_batch = torch.from_numpy(np.array(X_batch))\n",
    "    y_batch = torch.from_numpy(np.array(y_batch))\n",
    "    return X_batch, y_batch\n",
    "\n",
    "def create_val(batch_mask):\n",
    "    X_val=[]\n",
    "    y_val=[]\n",
    "    for i in range(len(batch_mask)):\n",
    "        case, x, y, z, l = data_val[batch_mask[i]]\n",
    "        case1 = case[:2]\n",
    "        case2 = case[3:]\n",
    "        X_val.append(f[case1][case2][:, x-16:x+17, y-16:y+17, z])\n",
    "        y_val.append(l)\n",
    "    X_val = torch.from_numpy(np.array(X_val))\n",
    "    y_val = torch.from_numpy(np.array(y_val))\n",
    "    return X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76704\n",
      "TwoPathConv (\n",
      "  (upper_layer1): Sequential (\n",
      "    (0): Conv2d(4, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(4, 4), stride=(1, 1), dilation=(1, 1))\n",
      "  )\n",
      "  (upper_layer2): Sequential (\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(2, 2), stride=(1, 1), dilation=(1, 1))\n",
      "  )\n",
      "  (under_layer1): Sequential (\n",
      "    (0): Conv2d(4, 160, kernel_size=(13, 13), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "  )\n",
      "  (final_layer): Conv2d(224, 5, kernel_size=(21, 21), stride=(1, 1))\n",
      ")\n",
      "batch_time: 1.845086000000009\n",
      "\n",
      "time used 2.436\n",
      "phase1 0.0% completed\n",
      "Variable containing:\n",
      " 1.6105\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 1)]\n",
      "\n",
      "Validation accuracy: 0.1972187104930468\n",
      "batch_time: 1.8378289999999993\n",
      "batch_time: 1.8515720000000044\n",
      "batch_time: 1.8254750000000115\n",
      "batch_time: 1.8293420000000253\n",
      "batch_time: 1.848540000000014\n",
      "batch_time: 1.8246469999999988\n",
      "batch_time: 1.80693100000002\n",
      "batch_time: 1.8235439999999983\n",
      "batch_time: 1.8245799999999974\n",
      "batch_time: 1.8329559999999958\n",
      "batch_time: 1.8509039999999857\n",
      "batch_time: 1.892629999999997\n",
      "batch_time: 1.8085279999999955\n",
      "batch_time: 1.818868000000009\n",
      "batch_time: 1.7893799999999942\n",
      "batch_time: 1.8055009999999925\n",
      "batch_time: 1.7943070000000034\n",
      "batch_time: 1.7840519999999742\n",
      "batch_time: 1.8086549999999875\n",
      "batch_time: 1.7822840000000042\n",
      "phase1 successfully trained!\n",
      "phase1 successfully saved!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "num_train=len(data_train_phase1)\n",
    "num_val=len(data_val)\n",
    "batch_size=900\n",
    "val_size=num_val\n",
    "num_epoch=8.0\n",
    "num_times=int(float(num_train)/batch_size*num_epoch)\n",
    "print (num_times)\n",
    "learning_rate = 5e-3\n",
    "reg=5e-7\n",
    "net = TwoPathConv()\n",
    "print(net)\n",
    "init_net(net)\n",
    "net.cuda(1)\n",
    "\n",
    "#create validation set\n",
    "val_mask=np.random.choice(num_val, val_size)\n",
    "X_val, y_val = create_val(val_mask)\n",
    "X_val = Variable(X_val.cuda(1), requires_grad=False)\n",
    "\n",
    "prev_time = time.clock()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=reg)\n",
    "net.zero_grad()\n",
    "for i in range(21):  # loop over the dataset multiple times\n",
    "    X_batch=None\n",
    "    batch_time=time.clock()\n",
    "    X_batch, y_batch = create_batch_phase1(i, batch_size)\n",
    "    print(\"batch_time: \"+ str(time.clock()-batch_time))\n",
    "    X_batch, y_batch = Variable(X_batch.cuda(1)), Variable(y_batch.cuda(1), requires_grad = False)\n",
    "    y_pred = net.forward(X_batch)\n",
    "    y_pred = y_pred.view(-1,5)\n",
    "    loss = F.cross_entropy(y_pred, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    net.zero_grad()\n",
    "    if i % 1000 == 0:\n",
    "        print (\"\")\n",
    "        print ('time used %.3f' % (time.clock()-prev_time))\n",
    "        print (\"phase1 \"+str(float(i)/num_times*100)+\"% completed\")\n",
    "        print (loss)\n",
    "        y_val_pred=net.forward(X_val)\n",
    "        y_val_pred=y_val_pred.view(-1,5)\n",
    "        useless, predicted=torch.max(y_val_pred.data, 1)\n",
    "        correct = (predicted == y_val.cuda(1)).sum()\n",
    "        print('Validation accuracy:', float(correct)/val_size)\n",
    "print (\"phase1 successfully trained!\")\n",
    "torch.save(net.state_dict(), \"diagnose_net.txt\")\n",
    "print (\"phase1 successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam(x, dx, config = None):\n",
    "    if config is None: \n",
    "        config = {}\n",
    "        config.setdefault('learning_rate',1e-4)\n",
    "        config.setdefault('beta1',0.9)\n",
    "        config.setdefault('beta2',0.999)\n",
    "        config.setdefault('epsilon',1e-8)\n",
    "        config.setdefault('m',torch.FloatTensor(x.size()).zero_().cuda(1))\n",
    "        config.setdefault('v',torch.FloatTensor(x.size()).zero_().cuda(1))\n",
    "        config.setdefault('t',0)\n",
    "    \n",
    "    next_x = None\n",
    "    config['t'] += 1\n",
    "    x.cuda(1)\n",
    "    dx.cuda(1)\n",
    "    config['m'] = config['beta1'] * config['m'] + (1 - config['beta1']) * dx\n",
    "    config['v'] = config['beta2'] * config['v'] + (1 - config['beta2']) * dx**2\n",
    "    mb = config['m'] / (1 - config['beta1'] ** config['t'])\n",
    "    vb = config['v'] / (1 - config['beta2'] ** config['t'])\n",
    "    next_x = x - config['learning_rate'] * mb / (np.sqrt(vb) + config['epsilon'])\n",
    "    return next_x, config\n",
    "\n",
    "def sgd_momentum(w, dw, config = None):\n",
    "\n",
    "    if config is None: \n",
    "        config = {}\n",
    "        config.setdefault('learning_rate', 5e-4)\n",
    "        config.setdefault('momentum', 0.9)\n",
    "\n",
    "    v = config.get('velocity', torch.FloatTensor(w.size()).zero_().cuda(1))\n",
    "    next_w = None\n",
    "\n",
    "    v = config['momentum'] * v - config['learning_rate'] * dw\n",
    "    next_w = w + v\n",
    "    config['velocity'] = v\n",
    "\n",
    "    return next_w, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131715\n",
      "\n",
      "time used 5.357\n",
      "0.0% completed\n",
      "Variable containing:\n",
      " 0.6691\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 1)]\n",
      "\n",
      "Validation accuracy: 0.8053097345132744\n",
      "\n",
      "time used 29.911\n",
      "0.0037960748585962115% completed\n",
      "Variable containing:\n",
      " 0.4948\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 1)]\n",
      "\n",
      "Validation accuracy: 0.8900126422250316\n",
      "\n",
      "time used 53.996\n",
      "0.007592149717192423% completed\n",
      "Variable containing:\n",
      " 0.3944\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 1)]\n",
      "\n",
      "Validation accuracy: 0.9190897597977244\n",
      "\n",
      "time used 78.599\n",
      "0.011388224575788634% completed\n",
      "Variable containing:\n",
      " 0.4175\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 1)]\n",
      "\n",
      "Validation accuracy: 0.9089759797724399\n",
      "\n",
      "time used 102.968\n",
      "0.015184299434384846% completed\n",
      "Variable containing:\n",
      " 0.3709\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 1)]\n",
      "\n",
      "Validation accuracy: 0.9152970922882427\n",
      "phase2 successfully trained!\n",
      "phase2 successfully saved!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "num_train=len(data_train_phase2)\n",
    "num_val=len(data_val)\n",
    "batch_size=900\n",
    "val_size=num_val\n",
    "num_epoch=5.0\n",
    "num_times=int(float(num_train)/batch_size*num_epoch)\n",
    "print (num_times)\n",
    "learning_rate = 5e-4\n",
    "reg=5e-7\n",
    "net.load_state_dict(torch.load(\"diagnose_net.txt\"))\n",
    "net.cuda(1)\n",
    "#create validation set\n",
    "val_mask=np.random.choice(num_val, val_size)\n",
    "X_val, y_val = create_val(val_mask)\n",
    "X_val = Variable(X_val.cuda(1), requires_grad=False)\n",
    "\n",
    "prev_time = time.clock()\n",
    "config_weight = None\n",
    "config_bias = None\n",
    "net.zero_grad()\n",
    "for i in range(num_times):  # loop over the dataset multiple times\n",
    "    X_batch = None\n",
    "    X_batch, y_batch = create_batch_phase2(i, batch_size)\n",
    "    X_batch, y_batch = Variable(X_batch.cuda(1)), Variable(y_batch.cuda(1), requires_grad = False)\n",
    "    y_pred = net.forward(X_batch)\n",
    "    y_pred = y_pred.view(-1,5)\n",
    "    loss = F.cross_entropy(y_pred, y_batch)\n",
    "    loss.backward()\n",
    "    dweight = net.final_layer.weight.grad.data+reg*net.final_layer.weight.data\n",
    "    dbias = net.final_layer.bias.grad.data\n",
    "    net.final_layer.weight.data, config_weight = sgd_momentum(net.final_layer.weight.data, dweight, config_weight)\n",
    "    net.final_layer.bias.data, config_bias = sgd_momentum(net.final_layer.bias.data, dbias, config_bias)\n",
    "    net.zero_grad()\n",
    "    if i % 1000 == 0:\n",
    "        print (\"\")\n",
    "        print ('time used %.3f' % (time.clock()-prev_time))\n",
    "        print (\"phase2+ \"str(float(i)/num_times*100)+\"% completed\")\n",
    "        print (loss)\n",
    "        y_val_pred=net.forward(X_val)\n",
    "        y_val_pred=y_val_pred.view(-1,5)\n",
    "        useless, predicted=torch.max(y_val_pred.data, 1)\n",
    "        correct = (predicted == y_val.cuda(1)).sum()\n",
    "        print('Validation accuracy:', float(correct)/val_size)\n",
    "print (\"phase2 successfully trained!\")\n",
    "torch.save(net.state_dict(), \"diagnose_net_phase2.txt\")\n",
    "print (\"phase2 successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoPathConv (\n",
      "  (upper_layer1): Sequential (\n",
      "    (0): Conv2d(4, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(4, 4), stride=(1, 1), dilation=(1, 1))\n",
      "  )\n",
      "  (upper_layer2): Sequential (\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(2, 2), stride=(1, 1), dilation=(1, 1))\n",
      "  )\n",
      "  (under_layer1): Sequential (\n",
      "    (0): Conv2d(4, 160, kernel_size=(13, 13), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "  )\n",
      "  (final_layer): Conv2d(224, 5, kernel_size=(21, 21), stride=(1, 1))\n",
      ")\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0960\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  0.1640\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      " -0.1697\n",
      "\n",
      "(0 ,3 ,.,.) = \n",
      " -0.1479\n",
      "\n",
      "(0 ,4 ,.,.) = \n",
      "  0.1214\n",
      "[torch.FloatTensor of size 1x5x1x1]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-db41910a458a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mcase1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mcase2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mX_train_phase1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcase1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcase2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0my_train_phase1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1496889914775/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1496889914775/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/yiqin/anaconda3/lib/python3.6/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# discards the array information at the top level.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mnew_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadtime_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mmtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;31m# === Special-case region references ====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoPathConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoPathConv, self).__init__()\n",
    "        self.upper_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(4,64,7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((4,4),stride = 1)\n",
    "        )\n",
    "        self.upper_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64,64,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2),stride = 1)\n",
    "        )\n",
    "        self.under_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(4,160,13),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final_layer = nn.Conv2d(224,5,21)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        upper_x = self.upper_layer2(self.upper_layer1(x))\n",
    "        under_x = self.under_layer1(x)\n",
    "        #upper_x = F.max_pool2d(F.relu(self.upper_conv1(x)), (4, 4),stride = 1)\n",
    "        #upper_x = F.max_pool2d(F.relu(self.upper_conv2(upper_x)), (2, 2), stride = 1)\n",
    "        #under_x = F.relu(self.under_conv1(x))\n",
    "        final_x = torch.cat((under_x, upper_x), 1)\n",
    "        out = self.final_layer(final_x)\n",
    "        return out\n",
    "        \n",
    "net = TwoPathConv()\n",
    "print(net)\n",
    "\n",
    "x = Variable(torch.randn(1,4,33,33), requires_grad = True)\n",
    "y_pred = net.forward(x)\n",
    "print(y_pred)\n",
    "#y_pred = y_pred.data.resize_(1,5)\n",
    "#y_pred = Variable(y_pred,requires_grad = True)\n",
    "#print(y_pred.size())\n",
    "#print(y_pred.view)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "import h5py\n",
    "f5 = h5py.File('training.h5','r')\n",
    "\n",
    "#get the training set for phase 1\n",
    "f=open(\"trainval-balanced.txt\", \"r\")\n",
    "content=f.readlines()\n",
    "X_train_phase1=[]\n",
    "y_train_phase1=[]\n",
    "for line in content:\n",
    "    no_n_line=line[0:len(line)-1]\n",
    "    item=no_n_line.split(\" \")\n",
    "    case, x, y, z, l = item[0], int(item[1]), int(item[2]), int(item[3]), int(item[4])\n",
    "    case1 = case[:2]\n",
    "    case2 = case[3:]\n",
    "    X_train_phase1.append(f5[case1][case2][:, x-16:x+17, y-16:y+17, z])\n",
    "    y_train_phase1.append(l)\n",
    "f.close()\n",
    "print (\"phase 1 data preparation process completed.\")\n",
    "\n",
    "#get the training set for phase 2 and the validation set\n",
    "data_val=[]\n",
    "f_in=open(\"trainval.txt\", \"r\")\n",
    "content=f_in.readlines()\n",
    "X_train_phase2=[]\n",
    "y_train_phase2=[]\n",
    "X_val=[]\n",
    "y_val=[]\n",
    "i=0\n",
    "for line in content:\n",
    "    no_n_line=line[0:len(line)-1]\n",
    "    item=no_n_line.split(\" \")\n",
    "    case, x, y, z, l = item[0], int(item[1]), int(item[2]), int(item[3]), int(item[4])\n",
    "    case1 = case[:2]\n",
    "    case2 = case[3:]\n",
    "    if i%30000==0:\n",
    "        X_val.append(f5[case1][case2][:, x-16:x+17, y-16:y+17, z])\n",
    "        y_val.append(l)\n",
    "    else:\n",
    "        X_train_phase2.append(f5[case1][case2][:, x-16:x+17, y-16:y+17, z])\n",
    "        y_train_phase2.append(l)\n",
    "    i += 1\n",
    "f_in.close()\n",
    "print (\"phase 2 data preparation process completed.\")\n",
    "\n",
    "f5.close()\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "print (len(X_train_phase1))\n",
    "print (len(y_train_phase1))\n",
    "print (len(X_train_phase1))\n",
    "print (len(y_train_phase2))\n",
    "print (len(X_val))\n",
    "print (len(y_val))\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "print (torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "import torch.nn.init as ninit\n",
    "def init_net(net):\n",
    "    for param in net.parameters():\n",
    "        ninit.uniform(param.data, a=-5e-3, b=5e-3)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "def create_batch_phase1(index, batch_size):\n",
    "    start=index*batch_size\n",
    "    end=start+batch_size\n",
    "    X_batch= X_train_phase1[start:end]\n",
    "    y_batch= y_train_phase1[start:end]\n",
    "    X_batch = torch.from_numpy(np.array(X_batch))\n",
    "    y_batch = torch.from_numpy(np.array(y_batch))\n",
    "    return X_batch, y_batch\n",
    "\n",
    "def create_batch_phase2(index, batch_size):\n",
    "    start=index*batch_size\n",
    "    end=start+batch_size\n",
    "    X_batch= X_train_phase2[start:end]\n",
    "    y_batch= y_train_phase2[start:end]\n",
    "    X_batch = torch.from_numpy(np.array(X_batch))\n",
    "    y_batch = torch.from_numpy(np.array(y_batch))\n",
    "    return X_batch, y_batch\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "num_train=len(data_train_phase1)\n",
    "num_val=len(data_val)\n",
    "batch_size=900\n",
    "val_size=num_val\n",
    "num_epoch=2.0\n",
    "num_times=int(float(num_train)/batch_size*num_epoch)\n",
    "print (num_times)\n",
    "learning_rate = 5e-3\n",
    "reg=5e-5\n",
    "net = TwoPathConv()\n",
    "print(net)\n",
    "init_net(net)\n",
    "net.cuda(0)\n",
    "\n",
    "#create validation set\n",
    "X_val = torch.from_numpy(np.array(X_val))\n",
    "y_val = torch.from_numpy(np.array(y_val))\n",
    "X_val = Variable(X_val.cuda(0), requires_grad=False)\n",
    "\n",
    "prev_time = time.clock()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=reg)\n",
    "net.zero_grad()\n",
    "for i in xrange(21):  # loop over the dataset multiple times\n",
    "    X_batch=None\n",
    "    batch_time=time.clock()\n",
    "    X_batch, y_batch = create_batch_phase1(i, batch_size)\n",
    "    print(\"batch_time: \"+ str(time.clock()-batch_time))\n",
    "    X_batch, y_batch = Variable(X_batch.cuda(0)), Variable(y_batch.cuda(0), requires_grad = False)\n",
    "    y_pred = net.forward(X_batch)\n",
    "    y_pred = y_pred.view(-1,5)\n",
    "    loss = F.cross_entropy(y_pred, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    net.zero_grad()\n",
    "    if i % 5 == 0:\n",
    "        print (\"\")\n",
    "        print ('time used %.3f' % (time.clock()-prev_time))\n",
    "        print (str(float(i)/num_times*100)+\"% completed\")\n",
    "        print (loss)\n",
    "        y_val_pred=net.forward(X_val)\n",
    "        y_val_pred=y_val_pred.view(-1,5)\n",
    "        useless, predicted=torch.max(y_val_pred.data, 1)\n",
    "        correct = (predicted == y_val.cuda(0)).sum()\n",
    "        print('Validation accuracy:', float(correct)/val_size)\n",
    "print (\"phase1 successfully trained!\")\n",
    "#torch.save(net.state_dict(), \"premature_net.txt\")\n",
    "print (\"phase1 successfully saved!\")\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "def adam(x, dx, config = None):\n",
    "    if config is None: \n",
    "        config = {}\n",
    "        config.setdefault('learning_rate',1e-4)\n",
    "        config.setdefault('beta1',0.9)\n",
    "        config.setdefault('beta2',0.999)\n",
    "        config.setdefault('epsilon',1e-8)\n",
    "        config.setdefault('m',torch.FloatTensor(x.size()).zero_().cuda(0))\n",
    "        config.setdefault('v',torch.FloatTensor(x.size()).zero_().cuda(0))\n",
    "        config.setdefault('t',0)\n",
    "    \n",
    "    next_x = None\n",
    "    config['t'] += 1\n",
    "    x.cuda(0)\n",
    "    dx.cuda(0)\n",
    "    config['m'] = config['beta1'] * config['m'] + (1 - config['beta1']) * dx\n",
    "    config['v'] = config['beta2'] * config['v'] + (1 - config['beta2']) * dx**2\n",
    "    mb = config['m'] / (1 - config['beta1'] ** config['t'])\n",
    "    vb = config['v'] / (1 - config['beta2'] ** config['t'])\n",
    "    next_x = x - config['learning_rate'] * mb / (np.sqrt(vb) + config['epsilon'])\n",
    "    return next_x, config\n",
    "\n",
    "def sgd_momentum(w, dw, config = None):\n",
    "\n",
    "    if config is None: \n",
    "        config = {}\n",
    "        config.setdefault('learning_rate', 5e-4)\n",
    "        config.setdefault('momentum', 0.9)\n",
    "\n",
    "    v = config.get('velocity', torch.FloatTensor(w.size()).zero_().cuda(0))\n",
    "    next_w = None\n",
    "\n",
    "    v = config['momentum'] * v - config['learning_rate'] * dw\n",
    "    next_w = w + v\n",
    "    config['velocity'] = v\n",
    "\n",
    "    return next_w, config\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "num_train=len(data_train_phase2)\n",
    "num_val=len(data_val)\n",
    "batch_size=900\n",
    "val_size=num_val\n",
    "num_epoch=2.0\n",
    "num_times=int(float(num_train)/batch_size*num_epoch)\n",
    "print (num_times)\n",
    "learning_rate = 5e-4\n",
    "reg=5e-5\n",
    "net.load_state_dict(torch.load(\"premature_net.txt\"))\n",
    "net.cuda(0)\n",
    "\n",
    "prev_time = time.clock()\n",
    "config_weight = None\n",
    "config_bias = None\n",
    "net.zero_grad()\n",
    "for i in xrange(21):  # loop over the dataset multiple times\n",
    "    X_batch = None\n",
    "    batch_time=time.clock()\n",
    "    X_batch, y_batch = create_batch_phase2(i, batch_size)\n",
    "    print(\"batch_time: \"+ str(time.clock()-batch_time))\n",
    "    X_batch, y_batch = Variable(X_batch.cuda(0)), Variable(y_batch.cuda(0), requires_grad = False)\n",
    "    y_pred = net.forward(X_batch)\n",
    "    y_pred = y_pred.view(-1,5)\n",
    "    loss = F.cross_entropy(y_pred, y_batch)\n",
    "    loss.backward()\n",
    "    dweight = net.final_layer.weight.grad.data+reg*net.final_layer.weight.data\n",
    "    dbias = net.final_layer.bias.grad.data\n",
    "    net.final_layer.weight.data, config_weight = sgd_momentum(net.final_layer.weight.data, dweight, config_weight)\n",
    "    net.final_layer.bias.data, config_bias = sgd_momentum(net.final_layer.bias.data, dbias, config_bias)\n",
    "    net.zero_grad()\n",
    "    if i % 5 == 0:\n",
    "        print (\"\")\n",
    "        print ('time used %.3f' % (time.clock()-prev_time))\n",
    "        print (str(float(i)/num_times*100)+\"% completed\")\n",
    "        print (loss)\n",
    "        y_val_pred=net.forward(X_val)\n",
    "        y_val_pred=y_val_pred.view(-1,5)\n",
    "        useless, predicted=torch.max(y_val_pred.data, 1)\n",
    "        correct = (predicted == y_val.cuda(0)).sum()\n",
    "        print('Validation accuracy:', float(correct)/val_size)\n",
    "print (\"phase2 successfully trained!\")\n",
    "torch.save(net.state_dict(), \"premature_net_phase2_without_biasreg.txt\")\n",
    "print (\"phase2 successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
